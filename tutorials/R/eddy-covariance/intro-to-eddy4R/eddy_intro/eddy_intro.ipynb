{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "syncID: 3857005e98a544a88a5e58625e32b514\n",
    "title: \"Introduction to working with NEON eddy flux data\"\n",
    "description: \"Download and navigate NEON eddy flux data, including basic transformations and merges\"\n",
    "dateCreated:  2019-07-09\n",
    "authors: Claire K. Lunch\n",
    "contributors: \n",
    "estimatedTime: 1 hour\n",
    "packagesLibraries: rhdf5, neonUtilities, ggplot2\n",
    "topics: HDF5, eddy-covariance, eddy-flux\n",
    "languagesTool: R\n",
    "dataProduct: DP4.00200.001\n",
    "code1: /R/eddy-intro/eddy_intro.r\n",
    "tutorialSeries: \n",
    "urlTitle: eddy-data-intro\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data tutorial provides an introduction to working with NEON eddy \n",
    "flux data, using the `neonUtilities` R package. If you are new to NEON \n",
    "data, we recommend starting with a more general tutorial, such as the \n",
    "<a href=\"https://www.neonscience.org/neonDataStackR\" target=\"_blank\">neonUtilities tutorial</a> \n",
    "or the <a href=\"https://www.neonscience.org/download-explore-neon-data\" target=\"_blank\">Download and Explore tutorial</a>. \n",
    "Some of the functions and techniques described in those tutorials will \n",
    "be used here, as well as functions and data formats that are unique to \n",
    "the eddy flux system.\n",
    "\n",
    "This tutorial assumes general familiarity with eddy flux data and \n",
    "associated concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Start by installing and loading packages and setting options. \n",
    "To work with the NEON flux data, we need the `rhdf5` package, \n",
    "which is hosted on Bioconductor, and requires a different \n",
    "installation process than CRAN packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('BiocManager')\n",
    "BiocManager::install('rhdf5')\n",
    "install.packages('neonUtilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(stringsAsFactors=F)\n",
    "\n",
    "library(neonUtilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `zipsByProduct()` function from the `neonUtilities` package to \n",
    "download flux data from two sites and two months. The transformations \n",
    "and functions below will work on any time range and site(s), but two \n",
    "sites and two months allows us to see all the available functionality \n",
    "while minimizing download size.\n",
    "\n",
    "Inputs to the `zipsByProduct()` function:\n",
    "\n",
    "* `dpID`: DP4.00200.001, the bundled eddy covariance product\n",
    "* `package`: basic (the expanded package is not covered in this tutorial)\n",
    "* `site`: NIWO = Niwot Ridge and HARV = Harvard Forest\n",
    "* `startdate`: 2018-06 (both dates are inclusive)\n",
    "* `enddate`: 2018-07 (both dates are inclusive)\n",
    "* `savepath`: modify this to something logical on your machine\n",
    "* `check.size`: T if you want to see file size before downloading, otherwise F\n",
    "\n",
    "The download may take a while, especially if you're on a slow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipsByProduct(dpID=\"DP4.00200.001\", package=\"basic\", \n",
    "              site=c(\"NIWO\", \"HARV\"), \n",
    "              startdate=\"2018-06\", enddate=\"2018-07\",\n",
    "              savepath=\"/Users/olearyd/Git/data/\", \n",
    "              check.size=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Levels\n",
    "\n",
    "There are five levels of data contained in the eddy flux bundle. For full \n",
    "details, refer to the <a href=\"http://data.neonscience.org/api/v0/documents/NEON.DOC.004571vA\" target=\"_blank\">NEON algorithm document</a>.\n",
    "\n",
    "Briefly, the data levels are:\n",
    "\n",
    "* Level 0' (dp0p): Calibrated raw observations\n",
    "* Level 1 (dp01): Time-aggregated observations, e.g. 30-minute mean gas concentrations\n",
    "* Level 2 (dp02): Time-interpolated data, e.g. rate of change of a gas concentration\n",
    "* Level 3 (dp03): Spatially interpolated data, i.e. vertical profiles\n",
    "* Level 4 (dp04): Fluxes\n",
    "\n",
    "The dp0p data are available in the expanded data package and are beyond \n",
    "the scope of this tutorial.\n",
    "\n",
    "The dp02 and dp03 data are used in storage calculations, and the dp04 data \n",
    "include both the storage and turbulent components. Since many users will \n",
    "want to focus on the net flux data, we'll start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Level 4 data (Fluxes!)\n",
    "\n",
    "To extract the Level 4 data from the HDF5 files and merge them into a \n",
    "single table, we'll use the `stackEddy()` function from the `neonUtilities` \n",
    "package.\n",
    "\n",
    "`stackEddy()` requires two inputs:\n",
    "\n",
    "* `filepath`: Path to a file or folder, which can be any one of:\n",
    "    1. A zip file of eddy flux data downloaded from the NEON data portal\n",
    "    2. A folder of eddy flux data downloaded by the `zipsByProduct()` function\n",
    "    3. The folder of files resulting from unzipping either of 1 or 2\n",
    "    4. A single HDF5 file of NEON eddy flux data\n",
    "* `level`: dp01-4\n",
    "\n",
    "Input the filepath you downloaded to using `zipsByProduct()` earlier, \n",
    "including the `filestoStack00200` folder created by the function, and \n",
    "`dp04`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux <- stackEddy(filepath=\"/Users/olearyd/Git/data/filesToStack00200/\",\n",
    "                 level=\"dp04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an object called `flux`. It's a named list containing four \n",
    "tables: one table for each site's data, and `variables` and `objDesc` \n",
    "tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of one of the site data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(flux$NIWO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `variables` and `objDesc` tables can help you interpret the column \n",
    "headers in the data table. The `objDesc` table contains definitions for \n",
    "many of the terms used in the eddy flux data product, but it isn't \n",
    "complete. To get the terms of interest, we'll break up the column headers \n",
    "into individual terms and look for them in the `objDesc` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term <- unlist(strsplit(names(flux$NIWO), split=\".\", fixed=T))\n",
    "flux$objDesc[which(flux$objDesc$Object %in% term),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the terms that aren't captured here, `fluxCo2`, `fluxH2o`, and `fluxTemp` \n",
    "are self-explanatory. The flux components are\n",
    "\n",
    "* `turb`: Turbulent flux\n",
    "* `stor`: Storage\n",
    "* `nsae`: Net surface-atmosphere exchange\n",
    "\n",
    "The `variables` table contains the units for each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux$variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some data! First, we'll need to convert the time stamps \n",
    "to an R date-time format (right now they're just character fields).\n",
    "\n",
    "### Time stamps\n",
    "\n",
    "NEON sensor data come with time stamps for both the start and end of \n",
    "the averaging period. Depending on the analysis you're doing, you may \n",
    "want to use one or the other; for general plotting, re-formatting, and \n",
    "transformations, I prefer to use the start time, because there \n",
    "are some small inconsistencies between data products in a few of the \n",
    "end time stamps.\n",
    "\n",
    "Note that **all** NEON data use UTC time, noted as \n",
    "`tz=\"GMT\"` in the code below. This is true across NEON's instrumented, \n",
    "observational, and airborne measurements. When working with NEON data, \n",
    "it's best to keep everything in UTC as much as possible, otherwise it's \n",
    "very easy to end up with data in mismatched times, which can cause \n",
    "insidious and hard-to-detect problems. Be sure to include the `tz` \n",
    "argument in all the lines of code below - if there is no time zone \n",
    "specified, R will default to the local time zone it detects on your \n",
    "operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeB <- as.POSIXct(flux$NIWO$timeBgn, \n",
    "                    format=\"%Y-%m-%dT%H:%M:%S\", \n",
    "                    tz=\"GMT\")\n",
    "flux$NIWO <- cbind(timeB, flux$NIWO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, \n",
    "     pch=\".\", xlab=\"Date\", ylab=\"CO2 flux\",\n",
    "     xaxt=\"n\")\n",
    "axis.POSIXct(1, x=timeB, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a lot of flux data, these data have some stray spikes, but there \n",
    "is a clear diurnal pattern going into the growing season.\n",
    "\n",
    "Let's trim down to just two days of data to see a few other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, \n",
    "     pch=20, xlab=\"Date\", ylab=\"CO2 flux\",\n",
    "     xlim=c(as.POSIXct(\"2018-07-07\", tz=\"GMT\"),\n",
    "            as.POSIXct(\"2018-07-09\", tz=\"GMT\")),\n",
    "    ylim=c(-20,20), xaxt=\"n\")\n",
    "axis.POSIXct(1, x=timeB, format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the timing of C uptake; the UTC time zone is clear here, where \n",
    "uptake occurs at times that appear to be during the night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge flux data with other sensor data\n",
    "\n",
    "Many of the data sets we would use to interpret and model flux data are \n",
    "measured as part of the NEON project, but are not present in the eddy flux \n",
    "data product bundle. In this section, we'll download PAR data and merge \n",
    "them with the flux data; the steps taken here can be applied to any of the \n",
    "NEON instrumented (IS) data products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PAR data\n",
    "\n",
    "To get NEON PAR data, use the `loadByProduct()` function from the \n",
    "`neonUtilities` package. `loadByProduct()` takes the same inputs as \n",
    "`zipsByProduct()`, but it loads the downloaded data directly into the \n",
    "current R environment.\n",
    "\n",
    "Let's download PAR data matching the Niwot Ridge flux data. The inputs \n",
    "needed are:\n",
    "\n",
    "* `dpID`: DP1.00024.001\n",
    "* `site`: NIWO\n",
    "* `startdate`: 2018-06\n",
    "* `enddate`: 2018-07\n",
    "* `package`: basic\n",
    "* `avg`: 30\n",
    "\n",
    "The new input here is `avg=30`, which downloads only the 30-minute data. \n",
    "Since the flux data are at a 30-minute resolution, we can save on \n",
    "download time by disregarding the 1-minute data files (which are of course \n",
    "30 times larger). The `avg` input can be left off if you want to download \n",
    "all available averaging intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr <- loadByProduct(\"DP1.00024.001\", site=\"NIWO\", avg=30,\n",
    "                    startdate=\"2018-06\", enddate=\"2018-07\",\n",
    "                    package=\"basic\", check.size=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pr` is another named list, and again, metadata and units can be found in \n",
    "the `variables` table. The `PARPAR_30min` table contains a `verticalPosition` \n",
    "field. This field indicates the position on the tower, with 10 being the \n",
    "first tower level, and 20, 30, etc going up the tower.\n",
    "\n",
    "### Join PAR to flux data\n",
    "\n",
    "We'll connect PAR data from the tower top to the flux data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.top <- pr$PARPAR_30min[which(pr$PARPAR_30min$verticalPosition==\n",
    "                                max(pr$PARPAR_30min$verticalPosition)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loadByProduct()` automatically converts time stamps when it reads the \n",
    "data, so here we just need to indicate which time field to use to \n",
    "merge the flux and PAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeB <- pr.top$startDateTime\n",
    "pr.top <- cbind(timeB, pr.top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And merge the two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.pr <- merge(pr.top, flux$NIWO, by=\"timeB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fx.pr$data.fluxCo2.nsae.flux~fx.pr$PARMean,\n",
    "     pch=\".\", ylim=c(-20,20),\n",
    "     xlab=\"PAR\", ylab=\"CO2 flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in data in the eddy covariance bundle besides the \n",
    "net flux data, the rest of this tutorial will guide you through how to \n",
    "get those data out of the bundle.\n",
    "\n",
    "## 5. Vertical profile data (Level 3)\n",
    "\n",
    "The Level 3 (`dp03`) data are the spatially interpolated profiles of \n",
    "the rates of change of CO<sub>2</sub>, H<sub>2</sub>O, and temperature.\n",
    "Extract the Level 3 data from the HDF5 file using `stackEddy()` with \n",
    "the same syntax as for the Level 4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof <- stackEddy(filepath=\"/Users/olearyd/Git/data/filesToStack00200/\",\n",
    "                 level=\"dp03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(prof$NIWO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Un-interpolated vertical profile data (Level 2)\n",
    "\n",
    "The Level 2 data are interpolated in time but not in space. They \n",
    "contain the rates of change at the measurement heights.\n",
    "\n",
    "Again, they can be extracted from the HDF5 files using `stackEddy()` \n",
    "with the same syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.l2 <- stackEddy(filepath=\"/Users/olearyd/Git/data/filesToStack00200/\",\n",
    "                 level=\"dp02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(prof.l2$HARV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here, as in the PAR data, there is a `verticalPosition` field. \n",
    "It has the same meaning as in the PAR data, indicating the tower level of \n",
    "the measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calibrated raw data (Level 1)\n",
    "\n",
    "Level 1 (`dp01`) data are calibrated, and aggregated in time, but \n",
    "otherwise untransformed. Use Level 1 data for raw gas \n",
    "concentrations and atmospheric stable isotopes.\n",
    "\n",
    "Using `stackEddy()` to extract Level 1 data requires additional \n",
    "inputs. The Level 1 files are too large to simply pull out all the \n",
    "variables by default, and they include mutiple averaging intervals, \n",
    "which can't be merged. So two additional inputs are needed:\n",
    "\n",
    "* `avg`: The averaging interval to extract\n",
    "* `var`: One or more variables to extract\n",
    "\n",
    "What variables are available, at what averaging intervals? Another \n",
    "function in the `neonUtilities` package, `getVarsEddy()`, returns \n",
    "a list of HDF5 file contents. It requires only one input, a filepath \n",
    "to a single NEON HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars <- getVarsEddy(\"/Users/olearyd/Git/data/filesToStack00200/NEON.D01.HARV.DP4.00200.001.nsae.2018-07.basic.20201020T201317Z.h5\")\n",
    "head(vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to `var` can be any values from the `name` field in the table \n",
    "returned by `getVarsEddy()`. Let's take a look at CO<sub>2</sub> and \n",
    "H<sub>2</sub>O, <sup>13</sup>C in CO<sub>2</sub> and <sup>18</sup>O in \n",
    "H<sub>2</sub>O, at 30-minute aggregation. Let's look at Harvard Forest \n",
    "for these data, since deeper canopies generally have more interesting \n",
    "profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso <- stackEddy(filepath=\"/Users/olearyd/Git/data/filesToStack00200/\",\n",
    "               level=\"dp01\", var=c(\"rtioMoleDryCo2\",\"rtioMoleDryH2o\",\n",
    "                                   \"dlta13CCo2\",\"dlta18OH2o\"), avg=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iso$HARV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot vertical profiles of CO<sub>2</sub> and <sup>13</sup>C in CO<sub>2</sub> \n",
    "on a single day. \n",
    "\n",
    "Here, for convenience, instead of converting the time stamps \n",
    "to a time format, it's easy to use the character format to extract the ones \n",
    "we want using `grep()`. And discard the `verticalPosition` values that are \n",
    "string values - those are the calibration gases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso.d <- iso$HARV[grep(\"2018-06-25\", iso$HARV$timeBgn, fixed=T),]\n",
    "iso.d <- iso.d[-which(is.na(as.numeric(iso.d$verticalPosition))),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ggplot` is well suited to these types of data, let's use it to plot \n",
    "the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"ggplot2\")\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g <- ggplot(iso.d, aes(y=verticalPosition)) + \n",
    "  geom_path(aes(x=data.co2Stor.rtioMoleDryCo2.mean, \n",
    "                group=timeBgn, col=timeBgn)) + \n",
    "  theme(legend.position=\"none\") + \n",
    "  xlab(\"CO2\") + ylab(\"Tower level\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g <- ggplot(iso.d, aes(y=verticalPosition)) + \n",
    "  geom_path(aes(x=data.isoCo2.dlta13CCo2.mean, \n",
    "                group=timeBgn, col=timeBgn)) + \n",
    "  theme(legend.position=\"none\") + \n",
    "  xlab(\"d13C\") + ylab(\"Tower level\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legends are omitted for space, see if you can work out the times \n",
    "of day the different colors represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
